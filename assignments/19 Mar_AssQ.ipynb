{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`Feature Engineering-3`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale data to a specific range. In this technique, each feature is scaled to a fixed range of [0,1] by subtracting the minimum value of the feature and then dividing by the difference between the maximum and minimum values. The formula for Min-Max scaling is as follows:\n",
    "\n",
    "X_normalized = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "where X is the original feature value, X_min is the minimum value of the feature, X_max is the maximum value of the feature, and X_normalized is the rescaled value.\n",
    "\n",
    "Min-Max scaling is used in data preprocessing to bring different features onto the same scale, which can help in the performance of machine learning algorithms. By rescaling the data to a fixed range, Min-Max scaling reduces the impact of outliers and helps in better visualization of the data. It is commonly used in image processing, where pixel values need to be rescaled to a specific range for better analysis.\n",
    "\n",
    "Here's an example below of how to apply Min-Max scaling using Python:\n",
    "\n",
    "Suppose you have a dataset containing the ages of 10 people ranging from 20 to 70 years old. You want to scale the ages to a range of 0 to 1 using Min-Max scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Age\n",
      "0  0.00\n",
      "1  0.04\n",
      "2  0.10\n",
      "3  0.16\n",
      "4  0.20\n",
      "5  0.30\n",
      "6  0.40\n",
      "7  0.60\n",
      "8  0.80\n",
      "9  1.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a sample dataset\n",
    "df = pd.DataFrame({'Age': [20, 22, 25, 28, 30, 35, 40, 50, 60, 70]})\n",
    "\n",
    "# Instantiate the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert the array back to a dataframe\n",
    "df_scaled = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "# Print the scaled dataframe\n",
    "print(df_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the Min-Max scaler transforms the ages to a range of 0 to 1 by subtracting the minimum age (20) from each value and then dividing the result with difference between minimum age(20) and maximum age(70) which is 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization, is a feature scaling technique that rescales the features of a dataset to have unit norm (i.e., length or magnitude of 1). This technique is useful when the features of a dataset have different scales, and you want to ensure that each feature contributes equally to the analysis.\n",
    "\n",
    "To apply the Unit Vector technique, you first calculate the norm of each feature vector (i.e., the square root of the sum of the squared values of the feature vector). Then, you divide each value in the feature vector by the norm of that vector. This rescales the values in the feature vector to be between -1 and 1, and ensures that the length of the feature vector is 1.\n",
    "\n",
    "For example, suppose you have a dataset with two features, x and y. If you have a data point with values x = 3 and y = 4, the feature vector for that point is [3, 4]. To normalize this feature vector, you first calculate its norm:\n",
    "\n",
    "norm = sqrt(3^2 + 4^2) = 5\n",
    "\n",
    "Then, you divide each value in the feature vector by the norm:\n",
    "\n",
    "x_norm = 3/5 = 0.6\n",
    "y_norm = 4/5 = 0.8\n",
    "\n",
    "The normalized feature vector for this data point is [0.6, 0.8], which has length 1.\n",
    "\n",
    "The Unit Vector technique and Min-Max scaling are both feature scaling techniques used to normalize the features of a dataset. However, the methods differ in how they rescale the features.\n",
    "\n",
    "As I mentioned earlier, the Unit Vector technique rescales the features of a dataset to have unit norm (i.e., length or magnitude of 1). This ensures that each feature contributes equally to the analysis, regardless of its scale. This technique is useful when you want to preserve the direction of the data and focus on the relative importance of each feature.\n",
    "\n",
    "On the other hand, Min-Max scaling rescales the features to a fixed range, usually between 0 and 1. To apply Min-Max scaling, you first subtract the minimum value of the feature from each value in the feature vector, then divide the result by the range (i.e., the difference between the maximum and minimum values). This technique is useful when you want to normalize the data to a specific range and ensure that all features have the same scale.\n",
    "\n",
    "here's an example to illustrate the application of the Unit Vector technique in feature scaling:\n",
    "\n",
    "Suppose we have a dataset with two features: age and income. Age ranges from 18 to 65, while income ranges from 20,000 to 200,000. We want to scale these features so that they have equal weights in our analysis.\n",
    "\n",
    "First, we need to calculate the norm for each data point. For a data point with age = 25 and income = 50,000, the norm would be:\n",
    "\n",
    "norm = sqrt(age^2 + income^2) = sqrt(25^2 + 50000^2) = 50,001\n",
    "\n",
    "Next, we need to divide each feature by the norm to get the unit vector. For this data point, the unit vector would be:\n",
    "\n",
    "age_unit = age / norm = 25 / 50,001 = 0.0005\n",
    "income_unit = income / norm = 50,000 / 50,001 = 0.9999\n",
    "\n",
    "The unit vector represents the direction of the data point, while its length is always 1. By applying the Unit Vector technique, we've rescaled the features so that they have equal weights in our analysis, regardless of their original scales. We can now use these scaled features for any analysis or modeling we want to perform on the dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Principal component analysis`, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\n",
    "\n",
    "So, to sum up, the idea of PCA is simple â€” reduce the number of variables of a data set, while preserving as much information as possible.\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of large datasets while retaining the maximum amount of variation present in the data. It is a type of unsupervised learning that aims to identify patterns in the data by transforming the original variables into a new set of orthogonal variables called principal components.\n",
    "\n",
    "In PCA, the first principal component is defined as the direction of maximum variance in the data. Subsequent principal components are orthogonal to the previous ones and capture the remaining variance in the data. The goal of PCA is to find a new set of variables that capture the most important patterns in the data while discarding the least important ones.\n",
    "\n",
    "PCA is widely used in many fields such as image and signal processing, bioinformatics, finance, and social sciences to simplify complex datasets, remove noise, and identify underlying patterns.\n",
    "\n",
    "`Example` : let's consider an example of how PCA can be applied in the field of finance.\n",
    "\n",
    "Suppose you are a financial analyst and you want to analyze the performance of a portfolio consisting of several stocks. The portfolio has data on various variables such as the stock price, earnings, dividends, and so on. However, with so many variables, it can be difficult to gain insights into the underlying patterns in the data.\n",
    "\n",
    "By using PCA, you can reduce the dimensionality of the data and identify the most important variables that contribute to the performance of the portfolio. This can help you make better investment decisions and manage risk.\n",
    "\n",
    "For example, after applying PCA, you may find that the first principal component is strongly correlated with the overall market trend. This means that the performance of the portfolio is heavily influenced by the general market conditions. The second principal component may be related to the performance of a specific sector, such as technology stocks.\n",
    "\n",
    "By understanding these relationships, you can make informed decisions on how to adjust your portfolio to minimize risk and maximize returns. You may decide to reduce your exposure to the technology sector or hedge against market volatility to manage your risk.\n",
    "\n",
    "Overall, PCA is a powerful tool that can help you gain insights into complex datasets and make better decisions in many fields, including finance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a common technique used for feature extraction. Feature extraction is the process of selecting and transforming the original features of a dataset into a new set of features that better represent the underlying patterns in the data.\n",
    "\n",
    "PCA is a popular method for feature extraction because it can identify the most important features that explain the most variance in the data. This is achieved by transforming the original features into a new set of orthogonal variables called principal components. These principal components are ranked in order of importance, so the first principal component explains the most variance in the data, followed by the second principal component, and so on.\n",
    "\n",
    "By selecting only the most important principal components, PCA can reduce the dimensionality of the dataset while still retaining the most relevant information. This makes it easier to analyze and visualize the data, and can improve the performance of machine learning algorithms that are trained on the transformed dataset.\n",
    "\n",
    "In summary, PCA is a technique for feature extraction that can help identify the most important variables in a dataset and transform them into a new set of variables that better capture the underlying patterns in the data.\n",
    "\n",
    "PCA can be used for feature extraction by transforming a high-dimensional dataset into a lower-dimensional space while retaining the most important information. This is done by identifying the principal components of the dataset, which are new variables that are linear combinations of the original variables.\n",
    "\n",
    "Let's illustrate this concept with an `example`. \n",
    "\n",
    "Let's say you have a dataset with 1000 features representing different aspects of a product, such as its color, shape, size, material, etc. You want to use machine learning to predict whether or not a customer will purchase the product based on these features.\n",
    "\n",
    "Before applying machine learning algorithms, you can use PCA to perform feature extraction and select the most important features that explain the most variance in the data. This can help improve the performance of the machine learning algorithms and reduce the risk of overfitting.\n",
    "\n",
    "To apply PCA, you first standardize the data by scaling each feature to have a mean of 0 and a standard deviation of 1. This ensures that all features are on the same scale and have equal importance in the analysis.\n",
    "\n",
    "Next, you calculate the covariance matrix of the standardized data, which measures the relationships between the different features. Then, you use the eigendecomposition to calculate the principal components of the data.\n",
    "\n",
    "Each principal component is a linear combination of the original features and represents a different axis in the data. The first principal component explains the most variance in the data, the second explains the second-most variance, and so on.\n",
    "\n",
    "You can use the scree plot or cumulative variance plot to decide how many principal components to keep. For example, you might decide to keep the first 10 principal components, which explain 80% of the variance in the data.\n",
    "\n",
    "Finally, you can transform the original features into the new principal components, which represent the most important features in the data. These new features can then be used as input for machine learning algorithms.\n",
    "\n",
    "By using PCA for feature extraction, you can reduce the dimensionality of the dataset and improve the performance of machine learning algorithms by focusing on the most important features that explain the most variance in the data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a recommendation system for a food delivery service, we might use various features such as price, rating, and delivery time to make recommendations to users. However, these features might be measured on different scales, which could make some features more influential than others. For example, the rating might range from 1 to 5, while the delivery time might range from 10 minutes to 1 hour.\n",
    "\n",
    "To address this issue, we can use Min-Max scaling to preprocess the data. Min-Max scaling scales the values of each feature to a range between 0 and 1, where the minimum value of the feature is mapped to 0 and the maximum value is mapped to 1. The formula for Min-Max scaling is:\n",
    "\n",
    "scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "\n",
    "where:\n",
    "\n",
    "original_value is the original value of the feature\n",
    "min_value is the minimum value of the feature\n",
    "max_value is the maximum value of the feature\n",
    "Here's how we would use Min-Max scaling to preprocess the data:\n",
    "\n",
    "For each feature, find the minimum value and the maximum value across all the data points in the dataset.\n",
    "\n",
    "Apply the Min-Max scaling formula to each data point and each feature to scale the values to the range between 0 and 1.\n",
    "\n",
    "The scaled data can now be used as input for the recommendation system.\n",
    "\n",
    "For example, let's say we have a dataset with three features: price, rating, and delivery time. The price ranges from $5 to $20, the rating ranges from 1 to 5, and the delivery time ranges from 10 minutes to 1 hour. We can use Min-Max scaling to scale each feature to a range between 0 and 1:\n",
    "\n",
    "* For price:\n",
    "\n",
    "    * min_value = $5\n",
    "    * max_value = $20\n",
    "    * scaled_value = (original_value - $5) / ($20 - $5)\n",
    "\n",
    "* For rating:\n",
    "\n",
    "    * min_value = 1\n",
    "    * max_value = 5\n",
    "    * scaled_value = (original_value - 1) / (5 - 1)\n",
    "\n",
    "* For delivery time:\n",
    "\n",
    "    * min_value = 10 minutes\n",
    "    * max_value = 1 hour (60 minutes)\n",
    "    * scaled_value = (original_value - 10 minutes) / (60 minutes - 10 minutes)\n",
    "\n",
    "After applying Min-Max scaling, all the features are now on the same scale and can be used together to make recommendations in a more balanced way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a model to predict stock prices, we might have a dataset with many features, such as company financial data and market trends. However, using all of these features in a machine learning model can lead to the \"curse of dimensionality,\" where the model becomes overly complex and overfit to the training data. To address this issue, we can use Principal Component Analysis (PCA) to reduce the dimensionality of the dataset.\n",
    "\n",
    "Here's how we would use PCA to reduce the dimensionality of the dataset:\n",
    "\n",
    "1. Standardize the data: We first standardize the data by scaling each feature to have a mean of 0 and a standard deviation of 1. This ensures that all features are on the same scale and have equal importance in the analysis.\n",
    "\n",
    "2. Calculate the covariance matrix: We then calculate the covariance matrix of the standardized data, which measures the relationships between the different features.\n",
    "\n",
    "3. Perform eigendecomposition: We perform eigendecomposition on the covariance matrix to calculate the principal components of the data. Each principal component is a linear combination of the original features and represents a different axis in the data. The first principal component explains the most variance in the data, the second explains the second-most variance, and so on.\n",
    "\n",
    "4. Choose the number of principal components: We can use the scree plot or cumulative variance plot to decide how many principal components to keep. For example, we might decide to keep the first 10 principal components, which explain 80% of the variance in the data.\n",
    "\n",
    "5. Transform the data: Finally, we transform the original features into the new principal components, which represent the most important features in the data. These new features can then be used as input for machine learning algorithms.\n",
    "\n",
    "By using PCA to reduce the dimensionality of the dataset, we can improve the performance of machine learning algorithms by focusing on the most important features that explain the most variance in the data. This can help us build a more accurate and efficient model to predict stock prices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 5, 10, 15, 20]\n",
    "df = pd.DataFrame(data,columns=[\"values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler(feature_range=(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     values\n",
       "0 -1.000000\n",
       "1 -0.578947\n",
       "2 -0.052632\n",
       "3  0.473684\n",
       "4  1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = pd.DataFrame(min_max.fit_transform(df[[\"values\"]]),columns=df.columns)\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a dimensionality reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. The number of principal components to retain depends on the specific dataset and the purpose of the analysis. One way to choose the number of principal components is to visualize the data in a more interpretable way, by conducting a PCA and selecting the first two or three principal components for the visualization. Another way is to choose a subset of the principal components that can explain the most variance, typically at least 90% of the variance.\n",
    "\n",
    "In this case, we can choose the number of principal components to retain based on the purpose of the analysis and the amount of variance we want to retain. We can also use the elbow method to determine the number of principal components to retain. The elbow method involves plotting the explained variance as a function of the number of principal components and selecting the number of principal components at the elbow of the curve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
