{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`Boosting-2`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q1. What is Gradient Boosting Regression?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression (GBR) is a type of boosting algorithm used for regression problems. It is a machine learning technique that builds an ensemble of decision trees, where each tree is trained on the residuals (i.e., the difference between the actual and predicted values) of the previous tree. The final prediction of the GBR model is the sum of the predictions of all the trees in the ensemble.\n",
    "\n",
    "In GBR, the algorithm uses the gradient descent optimization method to minimize the loss function. The loss function used in GBR is typically the mean squared error (MSE), which measures the average squared difference between the actual and predicted values.\n",
    "\n",
    "GBR works by iteratively adding decision trees to the model, where each new tree is trained to correct the errors made by the previous trees. In each iteration, the algorithm calculates the negative gradient of the loss function with respect to the predicted values, and then fits a new tree to this gradient. The predicted values of the new tree are then added to the predictions of the previous trees, and the process is repeated for a specified number of iterations or until a convergence criteria is met.\n",
    "\n",
    "GBR has been shown to be an effective and powerful technique for regression problems, and is widely used in many applications, such as finance, healthcare, and e-commerce."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a regressing dataset\n",
    "from sklearn.datasets import make_regression\n",
    "X , y = make_regression(n_features=4,n_informative=2,\n",
    "                        random_state=0,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spltting the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , y_train , y_test = train_test_split(X,y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model\n",
    "regressor = GradientBoostingRegressor()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.15098908220385\n",
      "0.937947539336522\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error , r2_score\n",
    "print(mean_squared_error(y_test,y_pred))\n",
    "print(r2_score(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\" : [0.15,0.25,0.3],\n",
    "    \"n_estimators\" : [300,400,1000],\n",
    "    \"max_depth\" : [1,2,3,4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GridSearchCV(regressor, param_grid=params, scoring= \"r2\" , cv=5, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=1, n_estimators=300;, score=0.960 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=1, n_estimators=300;, score=0.973 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=1, n_estimators=300;, score=0.942 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=1, n_estimators=300;, score=0.988 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=1, n_estimators=300;, score=0.856 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=1, n_estimators=400;, score=0.960 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=1, n_estimators=400;, score=0.974 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=1, n_estimators=400;, score=0.944 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=1, n_estimators=400;, score=0.987 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=1, n_estimators=400;, score=0.860 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=1, n_estimators=1000;, score=0.961 total time=   0.8s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=1, n_estimators=1000;, score=0.974 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=1, n_estimators=1000;, score=0.948 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=1, n_estimators=1000;, score=0.984 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=1, n_estimators=1000;, score=0.870 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=2, n_estimators=300;, score=0.917 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=2, n_estimators=300;, score=0.944 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=2, n_estimators=300;, score=0.891 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=2, n_estimators=300;, score=0.979 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=2, n_estimators=300;, score=0.852 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=2, n_estimators=400;, score=0.924 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=2, n_estimators=400;, score=0.943 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=2, n_estimators=400;, score=0.891 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=2, n_estimators=400;, score=0.978 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=2, n_estimators=400;, score=0.861 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=2, n_estimators=1000;, score=0.925 total time=   1.0s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=2, n_estimators=1000;, score=0.945 total time=   0.9s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=2, n_estimators=1000;, score=0.892 total time=   0.8s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=2, n_estimators=1000;, score=0.978 total time=   0.9s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=2, n_estimators=1000;, score=0.853 total time=   0.8s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=3, n_estimators=300;, score=0.949 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=3, n_estimators=300;, score=0.918 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=3, n_estimators=300;, score=0.885 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=3, n_estimators=300;, score=0.977 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=3, n_estimators=300;, score=0.870 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=3, n_estimators=400;, score=0.965 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=3, n_estimators=400;, score=0.927 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=3, n_estimators=400;, score=0.886 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=3, n_estimators=400;, score=0.974 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=3, n_estimators=400;, score=0.831 total time=   0.4s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=3, n_estimators=1000;, score=0.962 total time=   0.9s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=3, n_estimators=1000;, score=0.926 total time=   0.9s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=3, n_estimators=1000;, score=0.886 total time=   0.9s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=3, n_estimators=1000;, score=0.978 total time=   0.8s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=3, n_estimators=1000;, score=0.867 total time=   0.9s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=4, n_estimators=300;, score=0.880 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=4, n_estimators=300;, score=0.861 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=4, n_estimators=300;, score=0.820 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=4, n_estimators=300;, score=0.904 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=4, n_estimators=300;, score=0.917 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=4, n_estimators=400;, score=0.875 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=4, n_estimators=400;, score=0.862 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=4, n_estimators=400;, score=0.818 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=4, n_estimators=400;, score=0.898 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=4, n_estimators=400;, score=0.913 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.15, max_depth=4, n_estimators=1000;, score=0.874 total time=   0.6s\n",
      "[CV 2/5] END learning_rate=0.15, max_depth=4, n_estimators=1000;, score=0.861 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.15, max_depth=4, n_estimators=1000;, score=0.816 total time=   0.8s\n",
      "[CV 4/5] END learning_rate=0.15, max_depth=4, n_estimators=1000;, score=0.897 total time=   0.8s\n",
      "[CV 5/5] END learning_rate=0.15, max_depth=4, n_estimators=1000;, score=0.871 total time=   0.8s\n",
      "[CV 1/5] END learning_rate=0.25, max_depth=1, n_estimators=300;, score=0.956 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.25, max_depth=1, n_estimators=300;, score=0.965 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=0.25, max_depth=1, n_estimators=300;, score=0.953 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.25, max_depth=1, n_estimators=300;, score=0.988 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.25, max_depth=1, n_estimators=300;, score=0.848 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.25, max_depth=1, n_estimators=400;, score=0.956 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.25, max_depth=1, n_estimators=400;, score=0.966 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.25, max_depth=1, n_estimators=400;, score=0.956 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.25, max_depth=1, n_estimators=400;, score=0.987 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.25, max_depth=1, n_estimators=400;, score=0.851 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.25, max_depth=1, n_estimators=1000;, score=0.957 total time=   0.7s\n",
      "[CV 2/5] END learning_rate=0.25, max_depth=1, n_estimators=1000;, score=0.963 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.25, max_depth=1, n_estimators=1000;, score=0.960 total time=   0.6s\n",
      "[CV 4/5] END learning_rate=0.25, max_depth=1, n_estimators=1000;, score=0.985 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.25, max_depth=1, n_estimators=1000;, score=0.850 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.25, max_depth=2, n_estimators=300;, score=0.909 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=0.25, max_depth=2, n_estimators=300;, score=0.933 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.25, max_depth=2, n_estimators=300;, score=0.926 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.25, max_depth=2, n_estimators=300;, score=0.975 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.25, max_depth=2, n_estimators=300;, score=0.841 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.25, max_depth=2, n_estimators=400;, score=0.907 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.25, max_depth=2, n_estimators=400;, score=0.933 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.25, max_depth=2, n_estimators=400;, score=0.926 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.25, max_depth=2, n_estimators=400;, score=0.972 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.25, max_depth=2, n_estimators=400;, score=0.841 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.25, max_depth=2, n_estimators=1000;, score=0.908 total time=   0.7s\n",
      "[CV 2/5] END learning_rate=0.25, max_depth=2, n_estimators=1000;, score=0.933 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.25, max_depth=2, n_estimators=1000;, score=0.926 total time=   0.8s\n",
      "[CV 4/5] END learning_rate=0.25, max_depth=2, n_estimators=1000;, score=0.974 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.25, max_depth=2, n_estimators=1000;, score=0.841 total time=   0.8s\n",
      "[CV 1/5] END learning_rate=0.25, max_depth=3, n_estimators=300;, score=0.946 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.25, max_depth=3, n_estimators=300;, score=0.926 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.25, max_depth=3, n_estimators=300;, score=0.852 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.25, max_depth=3, n_estimators=300;, score=0.962 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.25, max_depth=3, n_estimators=300;, score=0.845 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.25, max_depth=3, n_estimators=400;, score=0.946 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.25, max_depth=3, n_estimators=400;, score=0.923 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.25, max_depth=3, n_estimators=400;, score=0.853 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.25, max_depth=3, n_estimators=400;, score=0.966 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.25, max_depth=3, n_estimators=400;, score=0.852 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.25, max_depth=3, n_estimators=1000;, score=0.927 total time=   0.7s\n",
      "[CV 2/5] END learning_rate=0.25, max_depth=3, n_estimators=1000;, score=0.923 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.25, max_depth=3, n_estimators=1000;, score=0.853 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.25, max_depth=3, n_estimators=1000;, score=0.966 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.25, max_depth=3, n_estimators=1000;, score=0.812 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.25, max_depth=4, n_estimators=300;, score=0.931 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.25, max_depth=4, n_estimators=300;, score=0.871 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.25, max_depth=4, n_estimators=300;, score=0.818 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.25, max_depth=4, n_estimators=300;, score=0.916 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.25, max_depth=4, n_estimators=300;, score=0.938 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.25, max_depth=4, n_estimators=400;, score=0.886 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.25, max_depth=4, n_estimators=400;, score=0.865 total time=   0.4s\n",
      "[CV 3/5] END learning_rate=0.25, max_depth=4, n_estimators=400;, score=0.811 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.25, max_depth=4, n_estimators=400;, score=0.909 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.25, max_depth=4, n_estimators=400;, score=0.941 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.25, max_depth=4, n_estimators=1000;, score=0.905 total time=   0.8s\n",
      "[CV 2/5] END learning_rate=0.25, max_depth=4, n_estimators=1000;, score=0.874 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.25, max_depth=4, n_estimators=1000;, score=0.811 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.25, max_depth=4, n_estimators=1000;, score=0.910 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.25, max_depth=4, n_estimators=1000;, score=0.920 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=1, n_estimators=300;, score=0.930 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=1, n_estimators=300;, score=0.975 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=1, n_estimators=300;, score=0.950 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=1, n_estimators=300;, score=0.986 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=1, n_estimators=300;, score=0.867 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=1, n_estimators=400;, score=0.932 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=1, n_estimators=400;, score=0.975 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=1, n_estimators=400;, score=0.951 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=1, n_estimators=400;, score=0.985 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=1, n_estimators=400;, score=0.870 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=1, n_estimators=1000;, score=0.937 total time=   0.8s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=1, n_estimators=1000;, score=0.976 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=1, n_estimators=1000;, score=0.957 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=1, n_estimators=1000;, score=0.985 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=1, n_estimators=1000;, score=0.867 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=2, n_estimators=300;, score=0.889 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=2, n_estimators=300;, score=0.932 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=2, n_estimators=300;, score=0.908 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=2, n_estimators=300;, score=0.983 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=2, n_estimators=300;, score=0.851 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=2, n_estimators=400;, score=0.884 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=2, n_estimators=400;, score=0.933 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=2, n_estimators=400;, score=0.907 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=2, n_estimators=400;, score=0.982 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=2, n_estimators=400;, score=0.851 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=2, n_estimators=1000;, score=0.898 total time=   0.7s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=2, n_estimators=1000;, score=0.932 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=2, n_estimators=1000;, score=0.907 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=2, n_estimators=1000;, score=0.981 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=2, n_estimators=1000;, score=0.871 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=3, n_estimators=300;, score=0.927 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=3, n_estimators=300;, score=0.924 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=3, n_estimators=300;, score=0.867 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=3, n_estimators=300;, score=0.980 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=3, n_estimators=300;, score=0.919 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=3, n_estimators=400;, score=0.959 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=3, n_estimators=400;, score=0.913 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=3, n_estimators=400;, score=0.875 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=3, n_estimators=400;, score=0.978 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=3, n_estimators=400;, score=0.895 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=3, n_estimators=1000;, score=0.956 total time=   0.7s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=3, n_estimators=1000;, score=0.925 total time=   0.6s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=3, n_estimators=1000;, score=0.867 total time=   0.6s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=3, n_estimators=1000;, score=0.981 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=3, n_estimators=1000;, score=0.799 total time=   0.6s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=4, n_estimators=300;, score=0.891 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=4, n_estimators=300;, score=0.877 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=4, n_estimators=300;, score=0.808 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=4, n_estimators=300;, score=0.932 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=4, n_estimators=300;, score=0.916 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=4, n_estimators=400;, score=0.910 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=4, n_estimators=400;, score=0.880 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=4, n_estimators=400;, score=0.795 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=4, n_estimators=400;, score=0.934 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=4, n_estimators=400;, score=0.927 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=4, n_estimators=1000;, score=0.873 total time=   0.6s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=4, n_estimators=1000;, score=0.872 total time=   0.6s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=4, n_estimators=1000;, score=0.805 total time=   0.6s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=4, n_estimators=1000;, score=0.936 total time=   0.6s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=4, n_estimators=1000;, score=0.929 total time=   0.6s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.15, 0.25, 0.3],\n",
       "                         &#x27;max_depth&#x27;: [1, 2, 3, 4],\n",
       "                         &#x27;n_estimators&#x27;: [300, 400, 1000]},\n",
       "             scoring=&#x27;r2&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.15, 0.25, 0.3],\n",
       "                         &#x27;max_depth&#x27;: [1, 2, 3, 4],\n",
       "                         &#x27;n_estimators&#x27;: [300, 400, 1000]},\n",
       "             scoring=&#x27;r2&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={'learning_rate': [0.15, 0.25, 0.3],\n",
       "                         'max_depth': [1, 2, 3, 4],\n",
       "                         'n_estimators': [300, 400, 1000]},\n",
       "             scoring='r2', verbose=3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473892996547371"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best r2 score\n",
    "reg.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.15, 'max_depth': 1, 'n_estimators': 1000}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best hyperparameters\n",
    "reg.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q4. What is a weak learner in Gradient Boosting?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n Gradient Boosting, a weak learner is a simple model that is not very expressive and has low predictive power on its own, but can be combined with other weak learners to form a stronger model. In particular, a weak learner is a decision tree with few nodes or a linear regression model with a small number of features.\n",
    "\n",
    "In Gradient Boosting, the weak learner is trained to fit the negative gradient of the loss function at each iteration. The idea is that the weak learner will learn to correct the errors made by the previous trees, and improve the overall performance of the model.\n",
    "\n",
    "The key to the success of Gradient Boosting is that it is able to combine many weak learners to form a strong model. By iteratively adding weak learners and updating the predictions of the model, Gradient Boosting is able to reduce the bias of the model and increase its accuracy.\n",
    "\n",
    "The use of weak learners in Gradient Boosting also helps to reduce the risk of overfitting, as each weak learner only focuses on a small subset of the features, and therefore has limited capacity to memorize the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q5. What is the intuition behind the Gradient Boosting algorithm?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm is to iteratively add simple models (weak learners) to the ensemble, where each new model tries to correct the errors made by the previous models. The algorithm works by minimizing the loss function of the model, which measures the difference between the predicted and actual values of the target variable.\n",
    "\n",
    "The basic idea of Gradient Boosting is to fit the negative gradient of the loss function at each iteration, which corresponds to the residual error between the predicted and actual values. By adding a new weak learner that fits the negative gradient, the algorithm is able to correct the errors made by the previous models and improve the overall performance of the model.\n",
    "\n",
    "The algorithm combines the predictions of all the weak learners by adding them together, with each new weak learner adding a new dimension to the prediction space. As a result, the final prediction of the model is a combination of many weak learners, each of which focuses on a different aspect of the data.\n",
    "\n",
    "The Gradient Boosting algorithm has several advantages over other machine learning techniques, including the ability to handle non-linear relationships between the features and the target variable, and the ability to handle missing data and outliers. It is also able to handle large datasets and high-dimensional feature spaces, and can be applied to a wide range of machine learning problems, including classification, regression, and ranking."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative fashion. The general process of building an ensemble of weak learners in Gradient Boosting can be summarized as follows:\n",
    "\n",
    "1. Initialize the model: The first step in Gradient Boosting is to initialize the model with a simple model, such as a decision tree with a single node. This model is called the \"base learner\" or the \"starting model\".\n",
    "\n",
    "2. Calculate the residual errors: Once the starting model is trained, the algorithm calculates the residual errors by subtracting the predictions of the starting model from the actual values of the target variable.\n",
    "\n",
    "3. Train a weak learner: The next step is to train a new weak learner on the residual errors calculated in the previous step. The weak learner is typically a simple model, such as a decision tree with a small number of nodes or a linear regression model with a small number of features.\n",
    "\n",
    "4. Update the model: The weak learner is added to the model by multiplying its predictions by a small learning rate (typically between 0.01 and 0.1) and adding them to the predictions of the previous model. This process updates the predictions of the model and reduces the residual errors.\n",
    "\n",
    "5. Repeat: Steps 2-4 are repeated for a fixed number of iterations or until the residual errors converge to a small value.\n",
    "\n",
    "By iteratively adding weak learners and updating the predictions of the model, the Gradient Boosting algorithm is able to build a powerful ensemble of weak learners that work together to improve the overall performance of the model. The algorithm is able to handle a wide range of machine learning problems, including classification, regression, and ranking, and has been shown to be effective in many applications, such as finance, healthcare, and e-commerce."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical intuition behind the Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "1. Define the loss function: The first step is to define a differentiable loss function that measures the difference between the predicted and actual values of the target variable. The loss function is usually a function of the residuals, which are the differences between the predicted and actual values.\n",
    "\n",
    "2. Initialize the model: The starting model is initialized with a constant value, which is the mean of the target variable. This value represents the global mean of the target variable and serves as the baseline prediction for all the data points.\n",
    "\n",
    "3. Compute the negative gradient of the loss function: The negative gradient of the loss function is computed with respect to the predictions of the model. This gradient represents the direction in which the predictions of the model need to be adjusted in order to minimize the loss function.\n",
    "\n",
    "4. Fit a weak learner to the negative gradient: A new weak learner is fitted to the negative gradient of the loss function. The weak learner is typically a simple model, such as a decision tree with a small number of nodes or a linear regression model with a small number of features.\n",
    "\n",
    "5. Update the model: The predictions of the weak learner are multiplied by a small learning rate and added to the predictions of the previous model. This process updates the predictions of the model and reduces the residual errors.\n",
    "\n",
    "6. Repeat steps 3-5: Steps 3-5 are repeated for a fixed number of iterations or until the residual errors converge to a small value.\n",
    "\n",
    "7. Combine the weak learners: The final model is obtained by combining the predictions of all the weak learners, weighted by their learning rates.\n",
    "\n",
    "The intuition behind these steps is that by iteratively fitting a weak learner to the negative gradient of the loss function and combining the weak learners to form a strong ensemble model, the Gradient Boosting algorithm is able to improve the performance of the model and minimize the loss function. By combining the predictions of the weak learners, the algorithm is able to capture complex non-linear relationships between the features and the target variable, and handle a wide range of machine learning problems, including classification, regression, and ranking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
