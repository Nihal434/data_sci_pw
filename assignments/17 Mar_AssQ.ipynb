{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3453e3-94c7-42f9-9866-386c7d2ebd1f",
   "metadata": {},
   "source": [
    "# **`Feature Engineering-1`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91129260-9f11-48f0-95aa-3d5150d359ac",
   "metadata": {},
   "source": [
    "`Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b291f-de4c-4d93-a878-624ea47280f6",
   "metadata": {},
   "source": [
    "**`Missing values`** in a dataset refer to the absence of a value in one or more variables for a particular observation. This can happen for several reasons, such as data entry errors, incomplete surveys or questionnaires, or missing data due to technical issues.\n",
    "\n",
    "Handling missing values is essential because they can affect the accuracy and reliability of the statistical analysis and machine learning models. Missing values can lead to biased estimates of the parameters, reduce the statistical power of the analysis, and even lead to incorrect conclusions. Moreover, many machine learning algorithms cannot handle missing values and may produce errors or incorrect predictions.\n",
    "\n",
    "Some common methods for handling missing values in a dataset include:\n",
    "\n",
    "1. Removing observations with missing values: This method involves removing all observations that contain at least one missing value. However, this approach can reduce the sample size and may lead to biased estimates if the missing data is not missing at random.\n",
    "\n",
    "2. Removing variables with missing values: This method involves removing all variables that contain missing values. This approach may be appropriate if the missing values are only present in a small number of variables or if the variables are not essential for the analysis.\n",
    "\n",
    "3. Imputation: This method involves estimating the missing values based on the values of other variables or based on the average or median value of the variable. There are several methods of imputation, such as mean imputation, median imputation, regression imputation, and multiple imputation.\n",
    "\n",
    "4. Treat missing values as a separate category: This method is suitable for categorical variables and involves creating a new category for the missing values.\n",
    "\n",
    "The choice of the method for handling missing values depends on the amount and pattern of missing data, the type of analysis, and the specific requirements of the problem. Handling missing values appropriately can improve the quality and reliability of the analysis and machine learning models.\n",
    "\n",
    "There are several machine learning algorithms that are not affected by missing values in the data. Some of these algorithms include:\n",
    "\n",
    "1. `Decision trees`: Decision trees can handle missing values by simply ignoring them when splitting nodes.\n",
    "\n",
    "2. `Random forests`: Random forests are an ensemble of decision trees and can handle missing values in a similar way as decision trees.\n",
    "\n",
    "3. `K-nearest neighbors (KNN)`: KNN is a non-parametric algorithm that does not require any assumptions about the distribution of the data. It can handle missing values by using the available data points to calculate the distance between observations.\n",
    "\n",
    "4. `Naive Bayes`: Naive Bayes is a probabilistic algorithm that can handle missing values by ignoring them and calculating the probability of the class given the available features.\n",
    "\n",
    "5. `Support vector machines (SVM)`: SVM can handle missing values by defining a distance measure between observations that does not depend on the missing values.\n",
    "\n",
    "6. `Principal Component Analysis (PCA)`: PCA can handle missing values by using an estimation method such as expectation-maximization (EM) algorithm to estimate the missing values and then performing dimensionality reduction.\n",
    "\n",
    "It's important to note that while these algorithms are not affected by missing values, the performance of the algorithm may be impacted if there are a large number of missing values or if the missing values are not missing at random. Therefore, it's still important to handle missing values appropriately, regardless of the algorithm being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40058094-d0f1-4115-8cb4-13d831be9991",
   "metadata": {},
   "source": [
    "`Q2: List down techniques used to handle missing data. Give an example of each with python code.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d235bee8-266e-47da-9a30-8b7debdaea8a",
   "metadata": {},
   "source": [
    "Here are some common techniques used to handle missing data:\n",
    "\n",
    "1. Deletion: This method involves removing observations with missing values. It can be further divided into two subcategories:\n",
    " - Listwise deletion: This approach removes entire observations with missing values.\n",
    " - Pairwise deletion: This approach only removes missing values for specific variables.\n",
    "2. Mean/Median/Mode Imputation: This method involves replacing the missing values with the mean/median/mode of the available values for that variable.\n",
    "\n",
    "3. Regression Imputation: This method involves predicting the missing values of a variable using regression analysis.\n",
    "\n",
    "4. K-nearest neighbor imputation: This method involves using the values of the k-nearest neighbors to estimate the missing value.\n",
    "\n",
    "5. Multiple Imputation: This method involves imputing the missing values multiple times to create multiple datasets and then analyzing each dataset to obtain an average or pooled estimate.\n",
    "\n",
    "6. Data augmentation: This method involves creating new data points based on the available data to fill in the missing values.\n",
    "\n",
    "7. Treat missing values as a separate category: This method is suitable for categorical variables and involves creating a new category for the missing values.\n",
    "\n",
    "The choice of method for handling missing data depends on the type and amount of missing data, the distribution of the data, and the specific requirements of the problem. It's essential to handle missing data appropriately to avoid bias and improve the accuracy and reliability of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29fbef29-1b21-4c22-b32a-61acc3fb4ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 15)\n",
      "(182, 15)\n"
     ]
    }
   ],
   "source": [
    "#Examples:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#1. Deletion\n",
    "\n",
    "#list wise deletion\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "print(df.shape)\n",
    "df.dropna(inplace=True) # deletion\n",
    "print(df.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e70e04e1-5695-4c3e-92f1-1911b5d494d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 2)\n",
      "(712, 2)\n"
     ]
    }
   ],
   "source": [
    "#pairwise deletion\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "print(df[[\"age\",\"embarked\"]].shape)\n",
    "df.dropna(subset=['age', 'embarked'], inplace=True) #pairwise deletion\n",
    "print(df[[\"age\",\"embarked\"]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59f848ee-1768-4068-a762-5258c407560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before 2\n",
      "After 0\n"
     ]
    }
   ],
   "source": [
    "#2.mean/median/mode imputation\n",
    "df = sns.load_dataset(\"titanic\") #load data\n",
    "\n",
    "#Mean/Median/Mode Imputation:\n",
    "print(\"Before\",df.embarked.isnull().sum()) #original\n",
    "\n",
    "# Impute missing values with mode\n",
    "imputer = SimpleImputer(strategy='most_frequent') # strategy can be changed to mean or median \n",
    "#respectively for mean or median imputation\n",
    "\n",
    "df[\"embarked\"] = imputer.fit_transform(df[[\"embarked\"]])\n",
    "print(\"After\",df.embarked.isnull().sum()) #final result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fe3adca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 177\n",
      "After 0\n"
     ]
    }
   ],
   "source": [
    "# 3. Regression Imputation:\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Load the dataset\n",
    "df = sns.load_dataset(\"titanic\") #load data\n",
    "\n",
    "#before imputation\n",
    "print(\"before\",df.age.isnull().sum())\n",
    "# Impute missing values with regression\n",
    "imputer = IterativeImputer() #imputation using linear regression\n",
    "df[\"age\"] = imputer.fit_transform(df[[\"age\"]])\n",
    "print(\"After\",df.age.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0030a787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 177\n",
      "after 0\n"
     ]
    }
   ],
   "source": [
    "#4. K-nearest neighbor imputation:\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Load the dataset\n",
    "df = sns.load_dataset(\"titanic\") #load data\n",
    "\n",
    "#before imputation\n",
    "print(\"before\",df.age.isnull().sum())\n",
    "\n",
    "# Impute missing values with KNN\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "df[['age']] = imputer.fit_transform(df[['age']])\n",
    "\n",
    "print(\"after\",df.age.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83fff2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 177\n",
      "after 0\n"
     ]
    }
   ],
   "source": [
    "# 5. Multiple Imputation:\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Load the dataset\n",
    "df = sns.load_dataset(\"titanic\") #load data\n",
    "\n",
    "#before imputation\n",
    "print(\"before\",df.age.isnull().sum())\n",
    "\n",
    "# Impute missing values with KNN\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "df[['age']] = imputer.fit_transform(df[['age']])\n",
    "\n",
    "print(\"after\",df.age.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee5e8255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  6.0\n",
      "2  3.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "# 6. Data augmentation:\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8]})\n",
    "\n",
    "# Create new data points based on existing data to fill in missing values\n",
    "df_augmented = df.copy()\n",
    "df_augmented.loc[2, 'A'] = 3\n",
    "df_augmented.loc[1, 'B'] = 6\n",
    "\n",
    "print(df_augmented)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3238ddf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         A        B\n",
      "0      1.0      5.0\n",
      "1      2.0  missing\n",
      "2  missing      7.0\n",
      "3      4.0      8.0\n"
     ]
    }
   ],
   "source": [
    "# 7. Treat missing values as a separate category:\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8]})\n",
    "\n",
    "# Replace missing values with a new label \"missing\"\n",
    "df['A'] = df['A'].fillna('missing')\n",
    "df['B'] = df['B'].fillna('missing')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c2d7c40",
   "metadata": {},
   "source": [
    "`Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51e64b5e",
   "metadata": {},
   "source": [
    "`Imbalanced data` refers to a situation in which the distribution of classes in a classification dataset is not equal. In other words, one class may have significantly more instances than another class. For example, in a medical diagnosis dataset, the number of healthy patients may be much larger than the number of sick patients.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to biased and inaccurate machine learning models. The model may have a high accuracy rate but may be predicting only the majority class, ignoring the minority class. This can result in a high false negative rate, which means that the model fails to correctly identify the minority class, leading to potentially serious consequences.\n",
    "\n",
    "For example, in a credit card fraud detection dataset, the majority of the transactions may be legitimate, and only a small percentage may be fraudulent. If the model is trained on this imbalanced dataset without any handling techniques, it may classify all transactions as legitimate, resulting in a high false negative rate and allowing fraudulent transactions to go undetected.\n",
    "\n",
    "Therefore, handling imbalanced data is crucial in machine learning to ensure that the model performs well on all classes and avoids making biased predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97830d84",
   "metadata": {},
   "source": [
    "`Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7495b8d",
   "metadata": {},
   "source": [
    "`Up-sampling` and `down-sampling` are techniques used to `handle imbalanced data` in machine learning.\n",
    "\n",
    "Up-sampling is the process of randomly replicating minority class samples to increase their number to match the majority class samples. This technique can be used when the minority class has too few samples to represent the true distribution of the data, and the model needs more data to learn from.\n",
    "\n",
    "Down-sampling, on the other hand, is the process of randomly removing samples from the majority class to reduce its number to match the minority class samples. This technique can be used when the majority class has significantly more samples than the minority class, and the model is biased towards the majority class.\n",
    "\n",
    "Here's an example to illustrate when up-sampling and down-sampling may be required:\n",
    "\n",
    "Suppose we have a dataset for credit card fraud detection, where the majority class is legitimate transactions, and the minority class is fraudulent transactions. If the dataset contains 10,000 legitimate transactions and only 100 fraudulent transactions, the dataset is heavily imbalanced, with a class distribution of 99% legitimate transactions and 1% fraudulent transactions.\n",
    "\n",
    "In this case, the model may not be able to accurately predict fraudulent transactions since it has too few samples to learn from. To handle this situation, we can use up-sampling to increase the number of fraudulent transactions by randomly replicating them, for example, to 1,000 or more. This will balance the dataset and enable the model to learn from a more representative set of data.\n",
    "\n",
    "On the other hand, if the dataset contains 10,000 fraudulent transactions and only 100 legitimate transactions, the model may be biased towards the fraudulent transactions and predict all transactions as fraudulent. In this case, we can use down-sampling to reduce the number of fraudulent transactions to match the number of legitimate transactions, for example, to 100. This will balance the dataset and enable the model to learn from both classes equally.\n",
    "\n",
    "Overall, up-sampling and down-sampling are essential techniques to handle imbalanced data and ensure that the machine learning model can make accurate predictions on all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c535d",
   "metadata": {},
   "source": [
    "`Q5: What is data Augmentation? Explain SMOTE.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d75e7cf",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used to increase the size of a dataset by generating new data from the existing data while preserving its label. It is a common technique used to address the problem of limited data and improve the performance of machine learning models.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation technique used for handling imbalanced datasets. It is used to generate new synthetic samples of the minority class in the imbalanced dataset, thereby balancing the class distribution. The technique works by creating synthetic samples of the minority class by interpolating between existing minority class samples.\n",
    "\n",
    "Here's a step-by-step explanation of how SMOTE works:\n",
    "\n",
    "1. For each minority class sample, SMOTE selects k nearest neighbors from the minority class.\n",
    "2. It then randomly selects one of the k nearest neighbors and generates a new synthetic sample by interpolating between the selected minority class sample and the chosen nearest neighbor.\n",
    "3. The interpolation is done by taking the difference between each feature value of the two samples and multiplying it by a random number between 0 and 1. This random number is generated for each feature, resulting in a new synthetic sample that lies somewhere between the two original samples in the feature space.\n",
    "4. SMOTE repeats this process until the desired number of synthetic samples has been generated.\n",
    "Here's an example of how SMOTE can be implemented in Python using the imbalanced-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff0437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example code\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# instantiate SMOTE with desired sampling strategy\n",
    "sm = SMOTE(sampling_strategy='minority')\n",
    "\n",
    "# fit and apply SMOTE to the training data\n",
    "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3d31514",
   "metadata": {},
   "source": [
    "In this example, SMOTE is instantiated with a sampling strategy of 'minority', which means that the minority class will be over-sampled to match the number of samples in the majority class. The fit_resample method is then called on the training data, which applies SMOTE and returns the new resampled data, X_resampled and y_resampled.\n",
    "\n",
    "SMOTE is a powerful technique for handling imbalanced datasets, but it should be used with caution as it can sometimes generate synthetic samples that are unrealistic or noisy. It is important to carefully tune the parameters of SMOTE and evaluate its impact on the performance of the machine learning model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcfb51ca",
   "metadata": {},
   "source": [
    "`Q6: What are outliers in a dataset? Why is it essential to handle outliers?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ecfad72",
   "metadata": {},
   "source": [
    "Outliers are observations or data points in a dataset that are significantly different from the majority of the other observations or data points. Outliers can be caused by various factors, such as measurement errors, data entry errors, or true variations in the data.\n",
    "\n",
    "It is essential to handle outliers in a dataset because they can have a significant impact on the results of data analysis and modeling. Outliers can lead to incorrect statistical analysis, biased estimates of model parameters, and reduced accuracy of predictive models. Outliers can also skew the distribution of data and affect the overall performance of the model.\n",
    "\n",
    "Handling outliers is important to ensure that the model is accurate and reliable. There are several techniques for handling outliers, including:\n",
    "\n",
    "1. Trimming: This involves removing a fixed percentage of the extreme values from the dataset. For example, the top and bottom 5% of values can be removed.\n",
    "\n",
    "2. Winsorization: This involves replacing the extreme values with the nearest non-outlier values. For example, the top and bottom 5% of values can be replaced with the values at the 95th and 5th percentiles, respectively.\n",
    "\n",
    "3. Z-score method: This involves identifying outliers based on their distance from the mean of the dataset. Observations that are more than a certain number of standard deviations away from the mean can be considered outliers.\n",
    "\n",
    "4. Interquartile range (IQR) method: This involves identifying outliers based on their distance from the median of the dataset. Observations that are more than a certain number of IQRs away from the median can be considered outliers.\n",
    "\n",
    "Here's an example of how to remove outliers using the Z-score method in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00d43905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: (100,)\n",
      "Cleaned data: (98,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate some random data with outliers\n",
    "data = np.random.normal(0, 1, 100)\n",
    "data[0] = 10\n",
    "data[1] = -10\n",
    "\n",
    "# compute the z-scores of the data\n",
    "z_scores = (data - np.mean(data)) / np.std(data)\n",
    "\n",
    "# identify outliers based on z-scores\n",
    "outliers = np.abs(z_scores) > 3\n",
    "\n",
    "# remove outliers from the data\n",
    "cleaned_data = data[~outliers]\n",
    "\n",
    "print(\"Original data:\", data.shape)\n",
    "print(\"Cleaned data:\", cleaned_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5c3cdd",
   "metadata": {},
   "source": [
    "Here's an example of how to remove outliers using the Interquartile range (IQR) method in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79f56f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower bound -3.15719984876644\n",
      "upper bound 3.0847035295496186\n",
      "Original data: (100,)\n",
      "Cleaned data: (98,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate some random data with outliers\n",
    "data = np.random.normal(0, 1, 100)\n",
    "data[0] = 10\n",
    "data[1] = -10\n",
    "\n",
    "# compute the IQR of the data\n",
    "q1, q3 = np.percentile(data, [25, 75])\n",
    "iqr = q3 - q1\n",
    "\n",
    "# define the upper and lower bounds for outlier detection\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "\n",
    "# identify outliers based on the IQR method\n",
    "outliers = (data < lower_bound) | (data > upper_bound)\n",
    "\n",
    "# remove outliers from the data\n",
    "cleaned_data = data[~outliers]\n",
    "\n",
    "print(\"lower bound\",lower_bound)\n",
    "print(\"upper bound\",upper_bound)\n",
    "print(\"Original data:\", data.shape)\n",
    "print(\"Cleaned data:\", cleaned_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ab13476",
   "metadata": {},
   "source": [
    "`Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9c41d0d",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to handle missing data in customer data analysis. Here are a few:\n",
    "\n",
    "1. Deletion: This involves removing any rows or columns in the dataset that contain missing data. However, this approach should be used with caution as it can result in a loss of information and potentially biased results.\n",
    "\n",
    "2. Mean/median imputation: This involves replacing missing values with the mean or median value of the available data. This approach is simple to implement but can result in biased estimates if the missing data is not missing at random.\n",
    "\n",
    "3. Mode imputation: This involves replacing missing values with the mode (most frequent value) of the available data. This approach is also simple to implement but may not be appropriate for continuous data.\n",
    "\n",
    "4. Regression imputation: This involves using regression models to predict the missing values based on other variables in the dataset. This approach can provide accurate estimates but requires additional modeling and may be sensitive to model assumptions.\n",
    "\n",
    "5. Multiple imputation: This involves creating multiple imputed datasets, each with slightly different imputed values, and analyzing each dataset separately. This approach can provide more accurate estimates and a measure of uncertainty, but requires more computational resources.\n",
    "\n",
    "6. Interpolation: This involves estimating the missing values based on the values of neighboring data points. This approach is often used for time series data.\n",
    "\n",
    "Ultimately, the choice of which technique to use depends on the nature of the data, the extent of the missingness, and the specific analysis being conducted."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8790e83e",
   "metadata": {},
   "source": [
    "`Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b637b73a",
   "metadata": {},
   "source": [
    "There are several strategies that can be used to determine if missing data is missing at random or if there is a pattern to the missing data:\n",
    "\n",
    "1. `Check for missing data patterns`: One approach is to visually inspect the data for patterns in the missing data. This can be done using heatmaps or other visualization tools to see if the missing data is clustered in certain areas or if it appears to be randomly distributed throughout the dataset.\n",
    "\n",
    "2. `Test for missing data patterns`: Statistical tests can be used to check for patterns in the missing data. One approach is to test whether the missing data is related to other variables in the dataset using correlation or regression analysis.\n",
    "\n",
    "3. `Compare missing data patterns between groups`: If the dataset includes groups or subgroups, it can be useful to compare missing data patterns between the groups. If there are significant differences in the missing data patterns between the groups, this can suggest that the missing data is not missing at random.\n",
    "\n",
    "4. `Impute missing data`: Another approach is to impute the missing data using different imputation methods and compare the results. If the results are similar across multiple imputation methods, this can suggest that the missing data is missing at random.\n",
    "\n",
    "Ultimately, the choice of strategy will depend on the nature of the dataset and the extent of the missing data. It is often recommended to use multiple strategies to cross-validate the results and increase confidence in the conclusions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6033045",
   "metadata": {},
   "source": [
    "`Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0eceead2",
   "metadata": {},
   "source": [
    "When working with imbalanced datasets, where the majority of samples belong to one class, there are several strategies that can be used to evaluate the performance of a machine learning model. Here are a few approaches:\n",
    "\n",
    "1. `Confusion matrix`: A confusion matrix is a table that shows the true positive, true negative, false positive, and false negative rates of the model predictions. It provides an overall picture of the performance of the model.\n",
    "\n",
    "2. `Precision and recall`: Precision and recall are two metrics that are often used to evaluate the performance of binary classification models on imbalanced datasets. Precision measures the accuracy of the positive predictions, while recall measures the ability of the model to identify all positive cases. Both metrics are important, but which one to focus on depends on the specific application.\n",
    "\n",
    "3. `F1 score`: The F1 score is the harmonic mean of precision and recall and is a commonly used metric for evaluating the performance of binary classification models on imbalanced datasets. It provides a single score that balances both precision and recall.\n",
    "\n",
    "4. `ROC curve and AUC`: The receiver operating characteristic (ROC) curve is a plot of the true positive rate against the false positive rate at various classification thresholds. The area under the curve (AUC) provides an overall measure of the model's ability to discriminate between positive and negative cases.\n",
    "\n",
    "5. `Resampling techniques`: Resampling techniques such as oversampling or undersampling can be used to balance the dataset and improve the performance of the model. Techniques like SMOTE (Synthetic Minority Over-sampling Technique) can be used to generate synthetic data points from the minority class.\n",
    "\n",
    "It is essential to choose the right evaluation metrics and strategies that are relevant to the specific problem and dataset. Moreover, it is always a good practice to cross-validate the results using different evaluation metrics and strategies to ensure that the model's performance is not biased towards a particular approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6d41dc1",
   "metadata": {},
   "source": [
    "`Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db124b2e",
   "metadata": {},
   "source": [
    "To balance an unbalanced dataset and down-sample the majority class, we can use the following techniques:\n",
    "\n",
    "1. `Random undersampling`: Randomly removing samples from the majority class until the dataset is balanced. This technique is simple to implement but can result in the loss of important information.\n",
    "\n",
    "2. `Cluster centroids`: In this technique, the majority class is down-sampled by identifying clusters of the majority class and replacing them with their respective centroids. This method can preserve important information, but it may not be effective if there is significant overlap between the majority and minority classes.\n",
    "\n",
    "3. `Tomek links`: Tomek links are pairs of instances from different classes that are close to each other but have different labels. In this technique, the majority class is down-sampled by removing instances that form Tomek links. This method can remove noisy and borderline instances but may not be effective if there is significant overlap between the classes.\n",
    "\n",
    "4. `Synthetic Minority Over-sampling Technique (SMOTE)`: SMOTE is an oversampling technique that generates synthetic data points from the minority class. The technique works by selecting a random minority class data point and creating synthetic samples along the line segments that connect its nearest neighbors. SMOTE can be used to balance the dataset by oversampling the minority class.\n",
    "\n",
    "To down-sample the majority class after balancing the dataset, we can use any of the above techniques to create a balanced dataset, followed by randomly selecting a subset of the majority class samples to down-sample the majority class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b14fa93",
   "metadata": {},
   "source": [
    "`Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77ac0ee3",
   "metadata": {},
   "source": [
    "To balance an unbalanced dataset and up-sample the minority class, we can use the following techniques:\n",
    "\n",
    "1. `Random oversampling`: Randomly duplicating samples from the minority class until the dataset is balanced. This technique is simple to implement but can result in overfitting and the generation of noisy samples.\n",
    "\n",
    "2. `SMOTE (Synthetic Minority Over-sampling Technique)`: SMOTE is a popular oversampling technique that generates synthetic data points from the minority class. The technique works by selecting a random minority class data point and creating synthetic samples along the line segments that connect its nearest neighbors. SMOTE can be used to balance the dataset by oversampling the minority class.\n",
    "\n",
    "3. `ADASYN (Adaptive Synthetic Sampling)`: ADASYN is an extension of SMOTE that generates more synthetic data points for the minority class samples that are harder to learn. This method can be more effective than SMOTE if there is significant overlap between the classes.\n",
    "\n",
    "4. `Synthetic minority augmentation (SMA)`: SMA is another oversampling technique that generates synthetic data points from the minority class. The technique works by adding random noise to the minority class samples to create new synthetic samples. This technique can be more effective than SMOTE in some cases.\n",
    "\n",
    "To up-sample the minority class after balancing the dataset, we can use any of the above techniques to create a balanced dataset, followed by randomly selecting a subset of the minority class samples to up-sample the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f2f4c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
