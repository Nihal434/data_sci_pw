{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`Dimensionality Reduction-3`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach?\n",
    "Explain with an example.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvalues and eigenvectors are concepts in linear algebra that are often used in various applications such as principal component analysis, image compression, and machine learning. The eigenvectors and eigenvalues are obtained through a process called eigen-decomposition, which is used to decompose a matrix into its constituent parts.\n",
    "\n",
    "Eigenvalues are scalar values that represent how much a particular vector is scaled when a linear transformation is applied to it. They are obtained by solving the characteristic equation of the matrix, which is defined as the determinant of the matrix minus a scalar times the identity matrix. The eigenvalues are the values of the scalar for which this equation has a non-trivial solution.\n",
    "\n",
    "Eigenvectors, on the other hand, are non-zero vectors that are transformed by a matrix only by a scalar factor. They are the directions that are preserved by the matrix when it is applied to them. Eigenvectors associated with different eigenvalues are usually orthogonal to each other.\n",
    "\n",
    "Eigen-decomposition is a method of factorizing a square matrix into a product of its eigenvectors and eigenvalues. For a given square matrix A, its eigen-decomposition is given by:\n",
    "\n",
    "A = QΛQ^-1\n",
    "\n",
    "where Q is the matrix whose columns are the eigenvectors of A, Λ is the diagonal matrix whose entries are the eigenvalues of A, and Q^-1 is the inverse of Q.\n",
    "\n",
    "Eigen-decomposition has numerous applications, including principal component analysis, which is a technique used for dimensionality reduction. In PCA, the eigenvectors of the covariance matrix of the data are computed, and these eigenvectors form the principal components that capture the most variance in the data. The eigenvalues associated with each eigenvector represent the amount of variance captured by that principal component.\n",
    "\n",
    "For example, consider a 2D dataset with the following data points:\n",
    "\n",
    "(1, 1), (2, 2), (3, 3)\n",
    "\n",
    "The covariance matrix of this dataset is:\n",
    "\n",
    "[[1, 1],\n",
    "[1, 1]]\n",
    "\n",
    "The eigenvectors and eigenvalues of this matrix can be computed using eigen-decomposition. The eigenvalues are λ1 = 0 and λ2 = 2, and the corresponding eigenvectors are [1, -1] and [1, 1], respectively. These eigenvectors form the two principal components of the dataset. The eigenvector associated with the larger eigenvalue (in this case, [1, 1]) captures the most variance in the data and represents the direction along which the data is spread the most. The eigenvector associated with the smaller eigenvalue (in this case, [1, -1]) captures the least variance in the data and represents the direction along which the data is spread the least."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q2. What is eigen decomposition and what is its significance in linear algebra?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigen decomposition is a method of factorizing a square matrix into a set of eigenvectors and corresponding eigenvalues. In other words, it is the process of breaking down a matrix into its fundamental components. The decomposition takes the form:\n",
    "\n",
    "A = QΛQ^-1\n",
    "\n",
    "where A is a square matrix, Q is a matrix consisting of the eigenvectors of A, Λ is a diagonal matrix consisting of the eigenvalues of A, and Q^-1 is the inverse of Q.\n",
    "\n",
    "The eigenvectors and eigenvalues of a matrix play an important role in linear algebra because they help us understand the behavior of linear transformations. Eigenvectors are the vectors that, when multiplied by a matrix, are scaled by a scalar factor, and the corresponding scalar is called an eigenvalue. In other words, an eigenvector remains in its original direction, only its magnitude changes.\n",
    "\n",
    "Eigen decomposition is significant in linear algebra because it provides a powerful tool for solving a variety of problems. For example, it can be used to find the optimal basis for a set of vectors, to transform data into a more manageable form, or to solve systems of linear equations. It is also the basis for many other techniques in linear algebra and machine learning, such as principal component analysis, singular value decomposition, and spectral clustering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the\n",
    "Eigen-Decomposition approach? Provide a brief proof to support your answer.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A square matrix is diagonalizable using the Eigen-Decomposition approach if and only if it has n linearly independent eigenvectors, where n is the dimension of the matrix. The proof involves showing that the eigenvectors form a basis for the vector space in which the matrix operates.\n",
    "\n",
    "Proof:\n",
    "\n",
    "Suppose A is a square matrix with dimension n, and λ1, λ2, ..., λn are the eigenvalues of A, with corresponding eigenvectors v1, v2, ..., vn. To show that A is diagonalizable using the Eigen-Decomposition approach, we need to show that the eigenvectors form a basis for the vector space in which A operates.\n",
    "\n",
    "First, we show that the eigenvectors are linearly independent. Suppose there exists a set of scalars c1, c2, ..., cn such that c1v1 + c2v2 + ... + cnvn = 0. Multiplying both sides by A, we get:\n",
    "\n",
    "Ac1v1 + Ac2v2 + ... + Acnvn = 0\n",
    "\n",
    "Since each vi is an eigenvector, we can substitute λi vi for A vi:\n",
    "\n",
    "λ1c1v1 + λ2c2v2 + ... + λncnvn = 0\n",
    "\n",
    "This equation can only be satisfied if all the ci's are zero, since the eigenvectors are distinct. Hence, the eigenvectors are linearly independent.\n",
    "\n",
    "Next, we show that the eigenvectors span the vector space. Let w be any vector in the vector space. We can express w as a linear combination of the eigenvectors:\n",
    "\n",
    "w = d1v1 + d2v2 + ... + dnvn\n",
    "\n",
    "Multiplying both sides by A, we get:\n",
    "\n",
    "Aw = Ad1v1 + Ad2v2 + ... + Adnvn\n",
    "\n",
    "Substituting λi vi for A vi, we get:\n",
    "\n",
    "Aw = λ1d1v1 + λ2d2v2 + ... + λndnvn\n",
    "\n",
    "Thus, any vector w in the vector space can be expressed as a linear combination of the eigenvectors. Therefore, the eigenvectors span the vector space.\n",
    "\n",
    "Since the eigenvectors are linearly independent and span the vector space, they form a basis for the vector space. Therefore, A can be diagonalized using the Eigen-Decomposition approach.\n",
    "\n",
    "In conclusion, a square matrix is diagonalizable using the Eigen-Decomposition approach if and only if it has n linearly independent eigenvectors, where n is the dimension of the matrix. The proof involves showing that the eigenvectors form a basis for the vector space in which the matrix operates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach?\n",
    "How is it related to the diagonalizability of a matrix? Explain with an example.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectral theorem states that a symmetric matrix can be diagonalized using an orthogonal matrix. This is significant in the context of the Eigen-Decomposition approach because it allows us to decompose a symmetric matrix into a diagonal matrix and an orthogonal matrix of eigenvectors.\n",
    "\n",
    "In other words, if A is a symmetric matrix, we can find a diagonal matrix D and an orthogonal matrix P such that A = PDP^-1, where D contains the eigenvalues of A on its diagonal, and the columns of P are the eigenvectors of A.\n",
    "\n",
    "This diagonalization can be useful in a number of ways, including simplifying matrix operations, identifying the most important features or dimensions of a dataset, and transforming data into a new coordinate system where the dimensions are uncorrelated.\n",
    "\n",
    "Here's an example in Python using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [3.61803399 1.38196601]\n",
      "Eigenvectors: [[ 0.85065081 -0.52573111]\n",
      " [ 0.52573111  0.85065081]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a symmetric matrix\n",
    "A = np.array([[3, 1], [1, 2]])\n",
    "\n",
    "# Compute the eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "# Check that A can be diagonalized using the spectral theorem\n",
    "P = eigenvectors\n",
    "D = np.diag(eigenvalues)\n",
    "assert np.allclose(A, P @ D @ np.linalg.inv(P))\n",
    "\n",
    "# Print the results\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print(\"Eigenvectors:\", eigenvectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create a symmetric matrix A using NumPy, and then compute its eigenvalues and eigenvectors using the np.linalg.eig function. We then verify that A can be diagonalized using the spectral theorem by computing PDP^-1 and checking that it is equal to A.\n",
    "\n",
    "Finally, we print the eigenvalues and eigenvectors to see the result of the decomposition. The eigenvalues are the diagonal elements of D, and the eigenvectors are the columns of P."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q5. How do you find the eigenvalues of a matrix and what do they represent?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the eigenvalues of a matrix, we need to solve the characteristic equation:\n",
    "\n",
    "$det(A - \\lambda I) = 0$\n",
    "\n",
    "where A is the matrix, λ is the eigenvalue, and I is the identity matrix.\n",
    "\n",
    "The eigenvalues represent the values that, when multiplied by the corresponding eigenvectors, produce the same vector as if the matrix was applied to the eigenvector. In other words, they represent how the matrix stretches or shrinks the eigenvectors.\n",
    "\n",
    "For example, consider the matrix A = [[1, 2], [2, 1]]. To find its eigenvalues, we solve:\n",
    "\n",
    "$det(A - \\lambda I) = det\\begin{pmatrix}1-\\lambda & 2 \\ 2 & 1-\\lambda\\end{pmatrix} = (1-\\lambda)^2 - 4 = \\lambda^2 - 2\\lambda - 3 = 0$\n",
    "\n",
    "The roots of this equation are λ1 = 3 and λ2 = -1. These are the eigenvalues of the matrix A.\n",
    "\n",
    "In numpy, you can find the eigenvalues of a matrix using the `linalg.eig()` function. The function returns two values: an array of eigenvalues and an array of eigenvectors. The eigenvalues are represented as a one-dimensional array, and each eigenvalue corresponds to one eigenvector in the eigenvectors array.\n",
    "\n",
    "The eigenvalues represent the scaling factor for each eigenvector. In other words, multiplying the eigenvector by its corresponding eigenvalue results in a vector that is parallel to the original vector, but with a different magnitude. This scaling factor is important in the context of the Eigen-Decomposition approach because it allows us to represent a matrix as a linear combination of its eigenvectors and eigenvalues."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q6. What are eigenvectors and how are they related to eigenvalues?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvectors are a special set of vectors that, when multiplied by a matrix, are scaled by a scalar value called the eigenvalue. More specifically, if A is a square matrix and v is a non-zero vector, then v is an eigenvector of A if and only if there exists a scalar λ such that:\n",
    "\n",
    "Av = λv\n",
    "\n",
    "In this equation, λ is the eigenvalue corresponding to the eigenvector v.\n",
    "\n",
    "Eigenvectors and eigenvalues are related because eigenvectors are the basis for the eigenspace associated with each eigenvalue. In other words, each eigenvalue has a corresponding eigenspace, which is the set of all eigenvectors that are scaled by that eigenvalue. The eigenspace associated with a particular eigenvalue is a subspace of the vector space that the matrix operates in.\n",
    "\n",
    "The importance of eigenvectors and eigenvalues lies in their use in the Eigen-Decomposition approach, which allows a square matrix to be decomposed into a set of eigenvectors and eigenvalues. This decomposition can be used in various applications, such as dimensionality reduction, image compression, and solving systems of linear differential equations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, eigenvectors and eigenvalues have a geometric interpretation that can be useful in understanding their properties and applications.\n",
    "\n",
    "The geometric interpretation of an eigenvector is that it represents a direction in which a linear transformation (represented by a matrix) stretches or compresses space. When a matrix is multiplied by an eigenvector, the result is a scaled version of the same eigenvector. The scaling factor is the corresponding eigenvalue. This means that the eigenvector is only scaled, not rotated or reflected, by the matrix.\n",
    "\n",
    "The geometric interpretation of an eigenvalue is that it represents the amount by which a linear transformation (represented by a matrix) stretches or compresses space along the corresponding eigenvector. If the eigenvalue is positive, then the transformation stretches space along the eigenvector. If the eigenvalue is negative, then the transformation compresses space along the eigenvector. If the eigenvalue is zero, then the transformation collapses space onto a lower-dimensional subspace.\n",
    "\n",
    "In summary, eigenvectors represent the directions in which a matrix only stretches or compresses space, while eigenvalues represent the amount of stretching or compression along those directions.\n",
    "\n",
    "Consider the following example where we have a 2D dataset with two variables x and y, and we want to find the eigenvectors and eigenvalues of its covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [13.33333333  0.        ]\n",
      "Eigenvectors: [[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create a 2D dataset\n",
    "data = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "\n",
    "# calculate the covariance matrix\n",
    "cov_matrix = np.cov(data.T)\n",
    "\n",
    "# find the eigenvectors and eigenvalues\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# print the results\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print(\"Eigenvectors:\", eigenvectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The np.cov function calculates the covariance matrix of the dataset, and the np.linalg.eig function finds the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "To visualize the eigenvectors and eigenvalues, we can plot them on a scatter plot along with the original dataset. Here is the code to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the dataset\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "\n",
    "# plot the eigenvectors\n",
    "plt.quiver(0, 0, eigenvectors[0,0], eigenvectors[1,0], color=['r'], scale=5)\n",
    "plt.quiver(0, 0, eigenvectors[0,1], eigenvectors[1,1], color=['b'], scale=5)\n",
    "\n",
    "# set the axis limits\n",
    "plt.xlim(-1, 9)\n",
    "plt.ylim(-1, 9)\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plt.scatter function plots the original dataset, and the plt.quiver function plots the eigenvectors as arrows. The x and y coordinates of each eigenvector correspond to its direction, and the length of the arrow is proportional to its corresponding eigenvalue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, the red arrow represents the first eigenvector, and the blue arrow represents the second eigenvector. The length of each arrow indicates the importance of that eigenvector, with longer arrows indicating larger eigenvalues. The eigenvectors represent the directions of maximum variance in the dataset, and the eigenvalues represent the amount of variance explained by each eigenvector."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q8. What are some real-world applications of eigen decomposition?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigen decomposition has several real-world applications in various fields such as:\n",
    "\n",
    "1. **Image Processing**: In image processing, eigen decomposition is used for image compression and feature extraction.\n",
    "\n",
    "2. **Recommender Systems**: In recommender systems, eigen decomposition is used for collaborative filtering, which helps in making personalized recommendations to users.\n",
    "\n",
    "3. **Finance**: In finance, eigen decomposition is used in portfolio optimization, where it helps in identifying the optimal combination of assets that will yield the highest return for a given level of risk.\n",
    "\n",
    "4. **Quantum Mechanics**: In quantum mechanics, eigen decomposition is used to find the energy levels of a quantum system.\n",
    "\n",
    "5. **Signal Processing**: In signal processing, eigen decomposition is used for noise reduction and data compression.\n",
    "\n",
    "6. **Machine Learning**: In machine learning, eigen decomposition is used in principal component analysis (PCA), which is a technique used for reducing the dimensionality of high-dimensional data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, a square matrix can have only one set of eigenvalues, but it can have multiple sets of eigenvectors corresponding to each eigenvalue.\n",
    "\n",
    "For example, consider a 2x2 matrix A with eigenvalues λ1 and λ2 and corresponding eigenvectors v1 and v2. If λ1 = λ2, then A has only one set of eigenvalues, i.e., λ1 = λ2 = λ. However, A may have multiple eigenvectors corresponding to λ. In fact, any linear combination of v1 and v2 will also be an eigenvector of A corresponding to λ.\n",
    "\n",
    "Therefore, while the set of eigenvalues is unique for a matrix, the set of eigenvectors corresponding to each eigenvalue is not unique and can be scaled or multiplied by a constant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning?\n",
    "Discuss at least three specific applications or techniques that rely on Eigen-Decomposition.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigen-Decomposition is a powerful tool in data analysis and machine learning that can help extract valuable insights from high-dimensional data. Here are three specific applications or techniques that rely on Eigen-Decomposition:\n",
    "\n",
    "1. **Principal Component Analysis (PCA)**: PCA is a popular technique for dimensionality reduction that involves finding the principal components of a dataset using Eigen-Decomposition. By projecting the data onto the principal components, we can reduce the dimensionality of the data while preserving the most important information. PCA has a wide range of applications in data analysis and machine learning, including image compression, data visualization, and feature extraction.\n",
    "\n",
    "2. **Recommender systems**: In collaborative filtering-based recommender systems, Eigen-Decomposition is used to factorize a user-item rating matrix into two lower-rank matrices, representing users and items, respectively. This technique, known as Singular Value Decomposition (SVD), can be used to predict missing ratings and recommend new items to users. SVD is widely used in e-commerce, music, and movie recommendation systems.\n",
    "\n",
    "3. **Graph analysis**: Eigen-Decomposition can be used to analyze the structure of networks or graphs. The largest eigenvalue of the adjacency matrix of a graph, known as the spectral radius, is a measure of the graph's connectivity or centrality. The eigenvectors corresponding to the largest eigenvalue can be used to identify important nodes or communities in the network. This technique has applications in social network analysis, bioinformatics, and information retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
